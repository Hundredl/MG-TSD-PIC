{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-wuyueying/miniconda3/envs/gluonts/lib/python3.8/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "create sol dataset csv\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='137')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "prepare the dataset\n",
      "len(train_data): 137, len(test_data): 959\n",
      "train_data.shape: (137, 7009)\n",
      "test_data.shape: (137, 7177)\n",
      "train_data_T.shape: (7009, 137)\n",
      "test_data_T.shape: (7177, 137)\n",
      "train_data_T[-1][:10]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "test_data_T[-1][:10]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "data_all.shape: (7177, 137)\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='137')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "df.shape: (7177, 137)\n",
      "train_len 6841\n",
      "valid_len 168\n",
      "test_len 168\n",
      "prediction_length 24\n",
      "./sol.csv saved\n",
      "--------------------------------------------------\n",
      "create elec dataset csv\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='370')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "prepare the dataset\n",
      "len(train_data): 370, len(test_data): 2590\n",
      "train_data.shape: (370, 5833)\n",
      "test_data.shape: (370, 4144)\n",
      "train_data_T.shape: (5833, 370)\n",
      "test_data_T.shape: (4144, 370)\n",
      "train_data_T[-1][:10]: [  52.419353  116.489365   41.19479    55.839725   41.0144     38.278774\n",
      "  426.7338    259.24472    13.347023 1000.      ]\n",
      "test_data_T[-1][:10]: [ 47.811058 114.3617    40.20385   54.987213  38.509705  35.90285\n",
      " 416.29382  285.70242   11.550308 993.6709  ]\n",
      "[  70.87912    50.406204  129.59381   137.21683   312.38745    66.61184\n",
      " 2525.216     114.91551   132.05821    59.470543]\n",
      "[  70.87912    50.406204  129.59381   137.21683   312.38745    66.61184\n",
      " 2525.216     114.91551   132.05821    59.470543]\n",
      "data_all.shape: (6001, 370)\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='370')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "df.shape: (6001, 370)\n",
      "train_len 5665\n",
      "valid_len 168\n",
      "test_len 168\n",
      "prediction_length 24\n",
      "./elec.csv saved\n",
      "--------------------------------------------------\n",
      "create traf dataset csv\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='963')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "prepare the dataset\n",
      "len(train_data): 963, len(test_data): 6741\n",
      "train_data.shape: (963, 4001)\n",
      "test_data.shape: (963, 4144)\n",
      "train_data_T.shape: (4001, 963)\n",
      "test_data_T.shape: (4144, 963)\n",
      "train_data_T[-1][:10]: [0.03025    0.12651667 0.05733333 0.07013334 0.10928334 0.13701667\n",
      " 0.09605    0.1986     0.07713334 0.12281667]\n",
      "test_data_T[-1][:10]: [0.02538333 0.12645    0.06191667 0.07233334 0.0882     0.2018\n",
      " 0.09605    0.30211666 0.09431667 0.11178333]\n",
      "[0.13183333 0.07373333 0.06638333 0.09635    0.03021667 0.07373333\n",
      " 0.07425    0.04795    0.08551667 0.16331667]\n",
      "[0.13183333 0.07373333 0.06638333 0.09635    0.03021667 0.07373333\n",
      " 0.07425    0.04795    0.08551667 0.16331667]\n",
      "data_all.shape: (4169, 963)\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='963')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "df.shape: (4169, 963)\n",
      "train_len 3833\n",
      "valid_len 168\n",
      "test_len 168\n",
      "prediction_length 24\n",
      "./traf.csv saved\n",
      "--------------------------------------------------\n",
      "create cup dataset csv\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='270')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=48\n",
      "prepare the dataset\n",
      "len(train_data): 270, len(test_data): 270\n",
      "train_data.shape: (270, 10872)\n",
      "test_data.shape: (270, 10920)\n",
      "train_data_T.shape: (10872, 270)\n",
      "test_data_T.shape: (10920, 270)\n",
      "train_data_T[-1][:10]: [58.935852  90.83032   50.393826   0.9673272 64.67714    9.296959\n",
      " 50.956314  93.820366  46.45189    0.762765 ]\n",
      "test_data_T[-1][:10]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 6.4 33.2 10.2  7.5 30.4  6.9 44.8 18.7 31.4 11. ]\n",
      "[ 6.4 33.2 10.2  7.5 30.4  6.9 44.8 18.7 31.4 11. ]\n",
      "data_all.shape: (10920, 270)\n",
      "metadata freq='H' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='270')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=48\n",
      "df.shape: (10920, 270)\n",
      "train_len 10824\n",
      "valid_len 48\n",
      "test_len 48\n",
      "prediction_length 48\n",
      "./cup.csv saved\n",
      "--------------------------------------------------\n",
      "create taxi dataset csv\n",
      "metadata freq='30min' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='1214')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "prepare the dataset\n",
      "len(train_data): 1214, len(test_data): 67984\n",
      "train_data.shape: (1214, 1488)\n",
      "test_data.shape: (1214, 1469)\n",
      "train_data_T.shape: (1488, 1214)\n",
      "test_data_T.shape: (1469, 1214)\n",
      "train_data_T[-1][:10]: [ 6.  8.  2.  7. 15.  5. 17. 16. 12. 12.]\n",
      "test_data_T[-1][:10]: [10.  1.  4.  4. 20.  6.  6. 10.  5. 12.]\n",
      "[16.  7.  4. 13. 25.  5.  4.  5. 12.  4.]\n",
      "[6. 5. 1. 1. 6. 6. 3. 3. 2. 3.]\n",
      "data_all.shape: (2856, 1214)\n",
      "metadata freq='30min' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='1214')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=24\n",
      "df.shape: (2856, 1214)\n",
      "train_len 1320\n",
      "valid_len 168\n",
      "test_len 1344\n",
      "prediction_length 24\n",
      "./taxi.csv saved\n",
      "--------------------------------------------------\n",
      "create wiki dataset csv\n",
      "metadata freq='D' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='9535')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=30\n",
      "prepare the dataset\n",
      "len(train_data): 9535, len(test_data): 47675\n",
      "train_data.shape: (2000, 762)\n",
      "test_data.shape: (2000, 912)\n",
      "train_data_T.shape: (762, 2000)\n",
      "test_data_T.shape: (912, 2000)\n",
      "train_data_T[-1][:10]: [6785. 1568. 1012. 2252. 1976. 2626. 1655.  866. 1334. 4769.]\n",
      "test_data_T[-1][:10]: [3566. 1052. 1267. 2384. 1155. 3587. 1529.  869. 1015. 3710.]\n",
      "[2435. 1535. 1035. 1074. 2376. 1624. 1843. 2666. 1747. 6210.]\n",
      "[2435. 1535. 1035. 1074. 2376. 1624. 1843. 2666. 1747. 6210.]\n",
      "data_all.shape: (912, 2000)\n",
      "metadata freq='D' target=None feat_static_cat=[CategoricalFeatureInfo(name='feat_static_cat_0', cardinality='9535')] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=30\n",
      "df.shape: (912, 2000)\n",
      "train_len 612\n",
      "valid_len 150\n",
      "test_len 150\n",
      "prediction_length 30\n",
      "./wiki.csv saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xbfT\\x97\\xf6\\xe1\\xcaZ\\x97\\xf7x\\xaf\\x9f\\x07P\\xdc\\x06\\xde\\xb5 ']\n",
      "Bad pipe message: %s [b'\\x19Y5\\xda\\x1a\\xe1\\x0cN\\x16\\xae\\xe2\\xc2\\xa1\\x1c\\xb0\\xab\\xd9\\x91 gw\\xc9\\x90\\xc7^tR\\x15\\x9c\\xa3\\xa9`/Nt\\x00\\x0f<\\xeb\\x1b\\xe8\\xe6\\x0b\\x13|q\\xe1GCc\\xc4\\x00\\x08']\n",
      "Bad pipe message: %s [b'\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00']\n",
      "Bad pipe message: %s [b\"~\\xb3\\x0c\\xf9\\xd2\\xd4\\xd5\\x93=\\xa8\\xcc\\x0bwy\\xe4\\x19\\xca\\n\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\"]\n",
      "Bad pipe message: %s [b'{\\xd8>\\xe8f\\xf1\\xd4\\xb1=,\\xf1w\\xd7D\\xeb\\xc8~P\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b'x\\x13\\xd6\\x9coj\\xe9\\xe0v\\xf1\\xe3\\xadE\\xa7\\xcb\\xa3)\\x0b\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00']\n",
      "Bad pipe message: %s [b'F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t']\n",
      "Bad pipe message: %s [b\"b\\xf2'\\xbe\\xd0I{\\xee\\xf9d\\xa9\\xc7\\x83\\xe7>lpl\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\"]\n",
      "Bad pipe message: %s [b'\\xff\\x97^\\xff*\\xe0>\\x1cu\\xea\\x8f\\x84\"\\x02K\\x05\\x90\\x16\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00']\n",
      "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n",
      "Bad pipe message: %s [b'\\x18=S\\xfc\\xf4\\xa9\\xc1\\xd3\\xa5 q\\xc4\\xd31']\n",
      "Bad pipe message: %s [b'\\xb8|\\x82z\"\\xe9\\x9a\\x81Wj\\xed\\xfb0`\\r\\xb5\\xa4p\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0\\'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00']\n",
      "Bad pipe message: %s [b'\\x11\\xc0\\x07\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b\"2\\xaa\\x0c\\xbe$\\\\\\xbcz\\x92>WB\\\\#)\\x1e-:\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\"]\n",
      "Bad pipe message: %s [b'A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\\x03\\x03\\x02\\x01\\x02']\n",
      "Bad pipe message: %s [b'\\x03']\n"
     ]
    }
   ],
   "source": [
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "\n",
    "\n",
    "def create_dataset_csv(dataset_simple,):\n",
    "    print('--------------------------------------------------')\n",
    "    print(f'create {dataset_simple} dataset csv')\n",
    "    dataset_alias = {\n",
    "        'sol':'solar_nips',\n",
    "        # 'fina':'m3_other',\n",
    "        'elec':'electricity_nips',\n",
    "        'traf':'traffic_nips',\n",
    "        'cup':'kdd_cup_2018_without_missing',\n",
    "        'taxi':'taxi_30min',\n",
    "        'wiki':'wiki-rolling_nips',\n",
    "        # 'exc':'exchange_rate_nips',\n",
    "        #  'fre':'fred_md',\n",
    "        }\n",
    "    \n",
    "    dataset_name = dataset_alias[dataset_simple]\n",
    "    dataset = get_dataset(dataset_name, regenerate=False)\n",
    "    metadata, train_data, test_data = dataset.metadata, dataset.train, dataset.test\n",
    "    print(\"metadata\", metadata)\n",
    "    train_grouper = MultivariateGrouper(max_target_dim=min(2000, int(dataset.metadata.feat_static_cat[0].cardinality)))\n",
    "    test_grouper = MultivariateGrouper(num_test_dates=int(len(dataset.test)/len(dataset.train)), \n",
    "                                    max_target_dim=min(2000, int(dataset.metadata.feat_static_cat[0].cardinality)))\n",
    "\n",
    "    print(\"prepare the dataset\")\n",
    "    print(f'len(train_data): {len(train_data)}, len(test_data): {len(test_data)}')\n",
    "\n",
    "    # if dataset_simple == 'taxi':\n",
    "    #     for i in train_data:\n",
    "    #         print(len(i['target']), i['start'], i['item_id'])\n",
    "    #         print(i)\n",
    "    #         break\n",
    "    #     for i in test_data:\n",
    "    #         print(len(i['target']), i['start'], i['item_id'])\n",
    "    #         print(i)\n",
    "    #         break\n",
    "    #     train_data = train_grouper(train_data)\n",
    "    #     test_data = test_grouper(test_data)\n",
    "    #     print(len(train_data), len(test_data))\n",
    "    #     print(test_data)\n",
    "    #     test_data = list(test_data)\n",
    "    #     for i in range(len(test_data)):\n",
    "    #         # print(len(i['target']), i['start'], i['item_id'])\n",
    "    #         print(len(test_data[i]['target'][0]))\n",
    "\n",
    "    # group the dataset\n",
    "    train_data=train_grouper(dataset.train)\n",
    "    if dataset_simple == 'cup': # cup的test数据长度不一致，需要处理\n",
    "        test_data = list(test_data)\n",
    "        for i in range(len(test_data)):\n",
    "            if len(test_data[i]['target']) == 10898:\n",
    "                # 补充8个0\n",
    "                test_data[i]['target'] = np.concatenate((test_data[i]['target'], np.zeros(8)))\n",
    "                # 去掉最后8个\n",
    "                # test_data[i]['target'] = test_data[i]['target'][:-8]\n",
    "            # print(len(test_data[i]['target']),test_data[i]['start'],test_data[i]['item_id'])\n",
    "        test_data = test_grouper(test_data)\n",
    "    else:\n",
    "        test_data=test_grouper(dataset.test)\n",
    "\n",
    "    # merge the train and test data\n",
    "    train_data = list(train_data)\n",
    "    test_data = list(test_data)\n",
    "    print(f'train_data.shape: {train_data[0][\"target\"].shape}')\n",
    "    print(f'test_data.shape: {test_data[-1][\"target\"].shape}')\n",
    "    train_data_T = np.array(train_data[0]['target']).T\n",
    "    test_data_T = np.array(test_data[-1]['target']).T\n",
    "    print(f'train_data_T.shape: {train_data_T.shape}')\n",
    "    print(f'test_data_T.shape: {test_data_T.shape}')\n",
    "\n",
    "    print(f'train_data_T[-1][:10]: {train_data_T[-1][:10]}')\n",
    "    print(f'test_data_T[-1][:10]: {test_data_T[-1][:10]}')\n",
    "\n",
    "\n",
    "    prediction_length = metadata.prediction_length\n",
    "    test_length = len(test_data)*prediction_length\n",
    "    if dataset_simple =='taxi':\n",
    "        # 与train 的部分没有重叠，train的start是2015-01-01 00:00:00，test的start是2016-01-01 00:00:00\n",
    "        test_data_T_unic = test_data_T[-test_length-prediction_length:]\n",
    "    else:\n",
    "        # 与train 的部分有重叠\n",
    "        test_data_T_unic = test_data_T[-test_length:]\n",
    "\n",
    "    print((train_data_T[-1][-10:]))\n",
    "    print((test_data_T[-len(test_data)*prediction_length-1][-10:]))\n",
    "    \n",
    "    data_all = np.concatenate((train_data_T, test_data_T_unic), axis=0)\n",
    "    print(f'data_all.shape: {data_all.shape}')\n",
    "\n",
    "    # generate dataframe\n",
    "    metadata = dataset.metadata\n",
    "    print(\"metadata\", metadata)\n",
    "    freq = metadata.freq\n",
    "\n",
    "    start = pd.Timestamp(\"2012-01-01 00:00:00\") # 开始时间 是 2012-01-01 00:00:00\n",
    "    index = pd.date_range(start=start, freq=freq, periods=len(data_all)) # 生成时间序列，间隔是freq，长度是len(data_all)\n",
    "    df = pd.DataFrame(data_all, index=index, columns=range(data_all.shape[1])) # 创建一个dataframe，index是时间序列，columns是0,1,2,3,4,5,6,7,8,9\n",
    "    df.index.name = 'date'\n",
    "    print(f'df.shape: {df.shape}')\n",
    "\n",
    "    test_len = len(test_data)*prediction_length\n",
    "    valid_len = min(7* prediction_length, test_len)\n",
    "    train_len = len(df) - test_len - valid_len\n",
    "\n",
    "    if dataset_simple == 'taxi':\n",
    "        train_len = len(df) - test_len - valid_len - prediction_length # test多添加了一部分，应该去掉\n",
    "\n",
    "    print(\"train_len\", train_len)\n",
    "    print(\"valid_len\", valid_len)\n",
    "    print(\"test_len\", test_len)\n",
    "    print(\"prediction_length\", prediction_length)\n",
    "\n",
    "    train_start = 0\n",
    "    train_end = train_start + train_len\n",
    "    valid_start = train_end\n",
    "    valid_end = valid_start + valid_len\n",
    "    test_start = valid_end\n",
    "    test_end = test_start + test_len\n",
    "    \n",
    "    \n",
    "    df.to_csv(f'./{dataset_simple}.csv', index=False)\n",
    "    print(f'./{dataset_simple}.csv saved')\n",
    "    return train_start, train_end, valid_start, valid_end, test_start, test_end, prediction_length\n",
    "\n",
    "dataset_split_index = {'dataset':[],'train_start':[], 'train_end':[], 'valid_start':[], 'valid_end':[], 'test_start':[], 'test_end':[], 'prediction_length':[]}\n",
    "transformed_dataset = ['sol', 'elec', 'traf', 'cup', 'taxi', 'wiki']\n",
    "# transformed_dataset = ['taxi', 'wiki']\n",
    "# transformed_dataset = ['cup']\n",
    "# transformed_dataset = ['taxi']\n",
    "for dataset in transformed_dataset:\n",
    "    train_start, train_end, valid_start, valid_end, test_start, test_end, prediction_length = create_dataset_csv(dataset)\n",
    "    dataset_split_index['dataset'].append(dataset)\n",
    "    dataset_split_index['train_start'].append(train_start)\n",
    "    dataset_split_index['train_end'].append(train_end)\n",
    "    dataset_split_index['valid_start'].append(valid_start)\n",
    "    dataset_split_index['valid_end'].append(valid_end)\n",
    "    dataset_split_index['test_start'].append(test_start)\n",
    "    dataset_split_index['test_end'].append(test_end)\n",
    "    dataset_split_index['prediction_length'].append(prediction_length)\n",
    "\n",
    "    dataset_split_index_df = pd.DataFrame(dataset_split_index)\n",
    "    dataset_split_index_df.to_csv('./dataset_split_index.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
